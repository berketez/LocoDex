# LocoDex vLLM Service

Bu servis, LocoDex projesi iÃ§in yÃ¼ksek performanslÄ± AI inference saÄŸlar. vLLM (Very Large Language Model) framework'Ã¼nÃ¼ kullanarak GPU hÄ±zlandÄ±rmalÄ± model Ã§alÄ±ÅŸtÄ±rma imkanÄ± sunar.

## Ã–zellikler

- ðŸš€ **YÃ¼ksek Performans**: vLLM ile optimize edilmiÅŸ inference
- ðŸ”¥ **GPU HÄ±zlandÄ±rma**: CUDA destekli GPU acceleration
- âš¡ **Tensor Parallelism**: Ã‡oklu GPU desteÄŸi
- ðŸŽ¯ **Quantization**: AWQ, GPTQ gibi quantization desteÄŸi
- ðŸ”„ **Streaming**: Real-time streaming responses
- ðŸ“Š **Monitoring**: Prometheus metrics ve sistem istatistikleri
- ðŸ”Œ **OpenAI Uyumluluk**: OpenAI API uyumlu endpoints

## Sistem Gereksinimleri

### Minimum
- NVIDIA GPU (CUDA destekli)
- 8GB+ GPU belleÄŸi
- Python 3.8+
- CUDA 11.8+

### Ã–nerilen
- NVIDIA RTX 4090, A100, H100
- 24GB+ GPU belleÄŸi
- Python 3.11
- CUDA 12.0+

## Kurulum

### Docker ile (Ã–nerilen)

```bash
# vLLM servisini baÅŸlat
docker-compose up vllm-service

# Logs kontrol et
docker-compose logs -f vllm-service
```

### Manuel Kurulum

```bash
# Gerekli paketleri yÃ¼kle
pip install -r requirements.txt

# Servisi baÅŸlat
python server.py
```

## KullanÄ±m

### 1. Model YÃ¼kleme

#### PopÃ¼ler Modeller

```bash
# Llama 2 7B Chat
curl -X POST http://localhost:8000/models/load \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "model_path": "meta-llama/Llama-2-7b-chat-hf",
      "tensor_parallel_size": 1,
      "gpu_memory_utilization": 0.85
    }
  }'

# Mistral 7B Instruct
curl -X POST http://localhost:8000/models/load \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "model_path": "mistralai/Mistral-7B-Instruct-v0.1",
      "tensor_parallel_size": 1
    }
  }'
```

#### Ã–zel YapÄ±landÄ±rma

```json
{
  "config": {
    "model_path": "model/path/or/hf/model",
    "tensor_parallel_size": 2,
    "max_model_len": 4096,
    "temperature": 0.7,
    "top_p": 0.9,
    "trust_remote_code": false,
    "gpu_memory_utilization": 0.85,
    "quantization": "awq"
  }
}
```

### 2. Chat API

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "current_model",
    "messages": [
      {"role": "user", "content": "Merhaba! Sen kimsin?"}
    ],
    "temperature": 0.7,
    "max_tokens": 1024
  }'
```

### 3. Completion API

```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "current_model",
    "prompt": "Yapay zeka hakkÄ±nda bir makale yaz:",
    "temperature": 0.7,
    "max_tokens": 1024
  }'
```

## API Endpoints

### Model YÃ¶netimi
- `GET /models` - YÃ¼klÃ¼ modelleri listele
- `POST /models/load` - Model yÃ¼kle
- `POST /models/unload` - Model kaldÄ±r

### Inference
- `POST /v1/chat/completions` - Chat completions
- `POST /v1/completions` - Text completions

### Monitoring
- `GET /health` - Servis saÄŸlÄ±k durumu
- `GET /stats` - Sistem istatistikleri
- `GET /metrics` - Prometheus metrics

## YapÄ±landÄ±rma

### Environment Variables

```bash
# Sunucu ayarlarÄ±
HOST=0.0.0.0
PORT=8000

# GPU ayarlarÄ±
CUDA_VISIBLE_DEVICES=0,1

# Redis baÄŸlantÄ±sÄ±
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=password

# Otomatik model yÃ¼kleme
AUTO_LOAD_MODEL=meta-llama/Llama-2-7b-chat-hf
```

### Model Parametreleri

| Parametre | AÃ§Ä±klama | VarsayÄ±lan |
|-----------|----------|------------|
| `model_path` | Model yolu (HuggingFace veya local) | - |
| `tensor_parallel_size` | GPU sayÄ±sÄ± | 1 |
| `max_model_len` | Max context uzunluÄŸu | Auto |
| `temperature` | YaratÄ±cÄ±lÄ±k derecesi | 0.7 |
| `top_p` | Nucleus sampling | 0.9 |
| `gpu_memory_utilization` | GPU bellek kullanÄ±mÄ± | 0.85 |
| `quantization` | Quantization tipi | None |

## Performance Tuning

### GPU Memory Optimization

```json
{
  "gpu_memory_utilization": 0.95,  // GPU belleÄŸinin %95'ini kullan
  "enforce_eager": false,          // CUDA graphs kullan
  "quantization": "awq"            // AWQ quantization
}
```

### Multi-GPU Setup

```json
{
  "tensor_parallel_size": 4,       // 4 GPU kullan
  "model_path": "meta-llama/Llama-2-70b-chat-hf"
}
```

## Monitoring

### Prometheus Metrics

- `vllm_requests_total` - Toplam istek sayÄ±sÄ±
- `vllm_request_duration_seconds` - Ä°stek sÃ¼resi
- `vllm_active_requests` - Aktif istek sayÄ±sÄ±
- `vllm_model_load_time_seconds` - Model yÃ¼kleme sÃ¼resi
- `vllm_gpu_memory_usage_bytes` - GPU bellek kullanÄ±mÄ±

### Health Check

```bash
curl http://localhost:8000/health
```

```json
{
  "status": "healthy",
  "model_loaded": true,
  "current_model": "meta-llama/Llama-2-7b-chat-hf",
  "gpu_count": 1,
  "memory_usage": {
    "cpu_percent": 15.3,
    "memory_percent": 68.7,
    "gpu_memory": [
      {
        "device": 0,
        "percent": 85.2,
        "allocated": 20480000000,
        "total": 24000000000
      }
    ]
  },
  "uptime": 3600.5
}
```

## Troubleshooting

### YaygÄ±n Sorunlar

1. **CUDA Out of Memory**
   ```bash
   # GPU bellek kullanÄ±mÄ±nÄ± azalt
   "gpu_memory_utilization": 0.7
   ```

2. **Model YÃ¼kleme HatasÄ±**
   ```bash
   # Model yolunu kontrol et
   "trust_remote_code": true  # Gerekirse
   ```

3. **YavaÅŸ Inference**
   ```bash
   # Tensor parallelism kullan
   "tensor_parallel_size": 2
   ```

### Loglar

```bash
# Docker logs
docker-compose logs -f vllm-service

# Manuel Ã§alÄ±ÅŸtÄ±rma
tail -f vllm.log
```

## LocoDex Entegrasyonu

vLLM servisi LocoDex ana uygulamasÄ±nda:

1. **Model Provider** olarak otomatik keÅŸfedilir
2. **vLLM sekmesi** ile yÃ¶netilir
3. **Chat interface**'de kullanÄ±labilir
4. **System monitoring**'de izlenir

### UI Features

- Model yÃ¼kleme ve kaldÄ±rma
- GPU kullanÄ±m izleme
- Performans metrikleri
- YapÄ±landÄ±rma yÃ¶netimi

## GÃ¼venlik

- Container izolasyonu
- Resource limiting
- Network segmentasyonu
- Input validation
- Rate limiting

## Lisans

Bu proje LocoDex lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.