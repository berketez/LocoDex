import asyncio
import aiohttp
import json
from datetime import datetime
import os
import logging

logger = logging.getLogger(__name__)

class RealDeepResearcher:
    """GerÃ§ek web aramasÄ± yapan deep research sistemi"""
    
    def __init__(self, model_name, model_source, websocket):
        self.model_name = model_name
        self.model_source = model_source
        self.websocket = websocket
        self.search_results = []
        
    async def call_local_model(self, prompt, system_prompt="", max_tokens=3000):
        """Lokal modeli asenkron olarak Ã§aÄŸÄ±rÄ±r - Ollama ve LM Studio desteÄŸi"""
        try:
            logger.info(f"call_local_model called with model_source: '{self.model_source}', model_name: '{self.model_name}'")
            import socket
            
            # Host IP'yi bul - Docker container'dan host'a eriÅŸim iÃ§in
            try:
                # Docker compose network'te host.docker.internal kullan
                host_ip = socket.gethostbyname('host.docker.internal')
            except:
                # Fallback IP'ler - macOS Docker Desktop iÃ§in
                try:
                    host_ip = "192.168.65.1"  # Docker Desktop default gateway
                except:
                    try:
                        host_ip = "172.17.0.1"  # Docker bridge network
                    except:
                        host_ip = "localhost"

            async with aiohttp.ClientSession() as session:
                logger.info(f"Model source comparison: '{self.model_source}' == 'Ollama' ? {self.model_source == 'Ollama'}")
                if self.model_source == "Ollama":
                    try:
                        # Model yÃ¼kleniyor bildirimi
                        await self.websocket.send_json({
                            "type": "progress", 
                            "step": 0, 
                            "message": f"ğŸ”„ {self.model_name} modeli yÃ¼kleniyor (ilk kullanÄ±mda zaman alabilir)..."
                        })
                        
                        ollama_url = f"http://{host_ip}:11434/api/generate"
                        ollama_payload = {
                            "model": self.model_name,
                            "prompt": f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:",
                            "stream": False,
                            "options": {
                                "temperature": 0.3,
                                "num_predict": max_tokens
                            }
                        }
                        
                        async with session.post(ollama_url, json=ollama_payload, timeout=300) as response:
                            logger.info(f"Ollama request to {ollama_url} with payload: {ollama_payload}")
                            logger.info(f"Ollama response status: {response.status}")
                            if response.status == 200:
                                data = await response.json()
                                return data.get('response', '').strip()
                            else:
                                error_text = await response.text()
                                logger.error(f"Ollama HTTP {response.status} error: {error_text}")
                                return f"Ollama hatasÄ±: {response.status} - {error_text}"
                    except Exception as ollama_error:
                        return f"Ollama hatasÄ±: {ollama_error}"
                
                elif self.model_source == "LM Studio":
                    try:
                        lm_studio_url = f"http://{host_ip}:1234/v1/chat/completions"
                        lm_payload = {
                            "model": self.model_name,
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": prompt}
                            ],
                            "temperature": 0.3,
                            "max_tokens": max_tokens,
                            "stream": False
                        }
                        
                        async with session.post(lm_studio_url, json=lm_payload, timeout=120) as response:
                            if response.status == 200:
                                data = await response.json()
                                content = data['choices'][0]['message']['content']
                                return content.strip()
                            else:
                                return f"LM Studio hatasÄ±: {response.status}"
                    except Exception as lm_error:
                        # LM Studio Ã§alÄ±ÅŸmÄ±yorsa Ollama'ya fallback yap
                        logger.warning(f"LM Studio eriÅŸilemez, Ollama'ya geÃ§iliyor: {lm_error}")
                        try:
                            ollama_url = f"http://{host_ip}:11434/api/generate"
                            ollama_payload = {
                                "model": self.model_name,
                                "prompt": f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:",
                                "stream": False,
                                "options": {
                                    "temperature": 0.3,
                                    "num_predict": max_tokens
                                }
                            }
                            
                            async with session.post(ollama_url, json=ollama_payload, timeout=300) as response:
                                if response.status == 200:
                                    data = await response.json()
                                    return data.get('response', '').strip()
                                else:
                                    return f"Hem LM Studio hem Ollama eriÅŸilemez"
                        except:
                            return f"Model baÄŸlantÄ± hatasÄ±: LM Studio Ã§alÄ±ÅŸmÄ±yor, Ollama'ya da eriÅŸilemedi"
                
                else: # Kaynak bilinmiyorsa Ã¶nce LM Studio dene, sonra Ollama
                    try:
                        lm_studio_url = f"http://{host_ip}:1234/v1/chat/completions"
                        lm_payload = {
                            "model": self.model_name,
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": prompt}
                            ],
                            "temperature": 0.3,
                            "max_tokens": max_tokens,
                            "stream": False
                        }
                        
                        async with session.post(lm_studio_url, json=lm_payload, timeout=120) as response:
                            if response.status == 200:
                                data = await response.json()
                                content = data['choices'][0]['message']['content']
                                return content.strip()
                    except:
                        pass
                    
                    try:
                        ollama_url = f"http://{host_ip}:11434/api/generate"
                        ollama_payload = {
                            "model": self.model_name,
                            "prompt": f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:",
                            "stream": False,
                            "options": {
                                "temperature": 0.3,
                                "num_predict": max_tokens
                            }
                        }
                        
                        async with session.post(ollama_url, json=ollama_payload, timeout=300) as response:
                            if response.status == 200:
                                data = await response.json()
                                return data.get('response', '').strip()
                    except:
                        pass
                
                return "Model baÄŸlantÄ± hatasÄ±: Hem LM Studio hem Ollama eriÅŸilemez"

        except Exception as e:
            return f"Model baÄŸlantÄ± hatasÄ±: {str(e)}"

    async def search_web(self, query, max_results=5):
        """Web aramasÄ± yapar - DuckDuckGo kullanarak"""
        try:
            await self.websocket.send_json({
                "type": "progress", 
                "step": 0.1, 
                "message": f"ğŸŒ Web'de '{query}' araÅŸtÄ±rÄ±lÄ±yor..."
            })
            
            # DuckDuckGo search async wrapper
            import asyncio
            import concurrent.futures
            
            def sync_search():
                from duckduckgo_search import DDGS
                ddgs = DDGS()
                return list(ddgs.text(query, max_results=max_results))
            
            # Run sync search in thread pool
            with concurrent.futures.ThreadPoolExecutor() as executor:
                search_results = await asyncio.get_event_loop().run_in_executor(
                    executor, sync_search
                )
            
            results = []
            for i, result in enumerate(search_results):
                if i >= max_results:
                    break
                    
                results.append({
                    'title': result.get('title', ''),
                    'body': result.get('body', ''),
                    'url': result.get('href', ''),
                    'source': 'DuckDuckGo'
                })
                
                await self.websocket.send_json({
                    "type": "progress", 
                    "step": 0.1 + (i * 0.05), 
                    "message": f"ğŸ“„ Kaynak bulundu: {result.get('title', '')[:60]}..."
                })
            
            return results
            
        except Exception as e:
            logger.error(f"Web search error: {e}")
            return []

    async def extract_content_from_url(self, url, title):
        """URL'den iÃ§erik Ã§eker"""
        try:
            await self.websocket.send_json({
                "type": "progress", 
                "step": 0.3, 
                "message": f"ğŸ“– {title[:50]}... sayfasÄ± okunuyor"
            })
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # Basit HTML temizleme
                        try:
                            from bs4 import BeautifulSoup
                            soup = BeautifulSoup(html, 'html.parser')
                        except ImportError:
                            # BeautifulSoup yoksa basit text extract
                            import re
                            # HTML taglerini kaldÄ±r
                            text = re.sub('<[^<]+?>', '', html)
                            return text[:3000]
                        
                        # Script ve style etiketlerini kaldÄ±r
                        for script in soup(["script", "style"]):
                            script.decompose()
                        
                        # Text'i al
                        text = soup.get_text()
                        
                        # SatÄ±rlarÄ± temizle
                        lines = (line.strip() for line in text.splitlines())
                        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                        text = ' '.join(chunk for chunk in chunks if chunk)
                        
                        # Ä°lk 3000 karakteri al
                        return text[:3000]
            
        except Exception as e:
            logger.error(f"Content extraction error for {url}: {e}")
            return ""
        
        return ""

    async def research_topic(self, topic):
        """GerÃ§ek deep research yapar"""
        
        await self.websocket.send_json({
            "type": "progress", 
            "step": 0.05, 
            "message": f"ğŸš€ '{topic}' konusu iÃ§in deep research baÅŸlÄ±yor..."
        })
        
        # 1. Konu analizi ve arama sorgularÄ± oluÅŸtur
        current_date = datetime.now().strftime("%d %B %Y")
        analysis_prompt = f"""
Bu araÅŸtÄ±rma konusunu analiz et ve 5 farklÄ± arama sorgusu Ã¶ner:

Konu: {topic}

BugÃ¼nÃ¼n tarihi: {current_date}
GÃ¼ncel geliÅŸmelere odaklan ve Ä°ngilizce arama sorgularÄ± Ã¼ret.

Her arama sorgusu farklÄ± bir aÃ§Ä±dan konuya yaklaÅŸmalÄ±. Sadece arama sorgularÄ±nÄ± listele, baÅŸka aÃ§Ä±klama yapma.
Format:
1. sorgu bir
2. sorgu iki
3. sorgu Ã¼Ã§
4. sorgu dÃ¶rt  
5. sorgu beÅŸ
"""
        
        await self.websocket.send_json({
            "type": "progress", 
            "step": 0.1, 
            "message": "ğŸ§  AraÅŸtÄ±rma stratejisi belirleniyor..."
        })
        
        try:
            logger.info(f"About to call call_local_model for analysis...")
            queries_text = await self.call_local_model(
                analysis_prompt, 
                "Sen araÅŸtÄ±rma uzmanÄ±sÄ±n. Verilen konular iÃ§in etkili arama sorgularÄ± oluÅŸturursun."
            )
            logger.info(f"call_local_model completed successfully")
        except Exception as e:
            logger.error(f"call_local_model failed: {e}")
            queries_text = "1. ana konu\n2. genel araÅŸtÄ±rma"
        
        # SorgularÄ± parse et - duplikasyonlarÄ± Ã¶nle
        search_queries = []
        seen_queries = set()
        lines = queries_text.split('\n')
        for line in lines:
            line = line.strip()
            if line and (line[0].isdigit() or line.startswith('-')):
                # NumarayÄ± kaldÄ±r
                query = line.split('.', 1)[-1].strip().lower()
                if query and query not in seen_queries and len(query) > 5:
                    search_queries.append(query)
                    seen_queries.add(query)
        
        # Fallback sorgularÄ± ekle
        if not search_queries:
            search_queries = [topic, f"{topic} nedir", f"{topic} hakkÄ±nda"]
        
        # Ä°lk 3 sorguyu kullan
        search_queries = search_queries[:3]
        
        # 2. Web aramasÄ± yap
        all_search_results = []
        
        for i, query in enumerate(search_queries):
            await self.websocket.send_json({
                "type": "progress", 
                "step": 0.15 + (i * 0.15), 
                "message": f"ğŸ” Arama {i+1}/{len(search_queries)}: {query[:50]}..."
            })
            
            results = await self.search_web(query, max_results=3)
            all_search_results.extend(results)
            
            await asyncio.sleep(1)  # Rate limiting
        
        # 3. Ä°Ã§erikleri analiz et
        research_data = []
        
        for i, result in enumerate(all_search_results[:8]):  # Ä°lk 8 sonuÃ§
            await self.websocket.send_json({
                "type": "progress", 
                "step": 0.5 + (i * 0.05), 
                "message": f"ğŸ“Š Kaynak analizi: {result['title'][:40]}..."
            })
            
            # Ä°Ã§erik Ã§ek
            if result['url']:
                content = await self.extract_content_from_url(result['url'], result['title'])
                result['content'] = content
            
            # Model ile analiz et
            if result.get('content') or result.get('body'):
                analysis_text = result.get('content', result.get('body', ''))
                
                analysis_prompt = f"""
Bu web kaynaÄŸÄ±ndaki bilgileri analiz et ve '{topic}' konusu ile ilgili Ã¶nemli bilgileri Ã¶zetle:

Kaynak: {result['title']}
URL: {result['url']}

Ä°Ã§erik:
{analysis_text[:2000]}

Sadece konuyla ilgili Ã¶nemli bilgileri Ã¶zetle, kaynak adÄ±nÄ± da belirt.
"""
                
                analysis = await self.call_local_model(
                    analysis_prompt,
                    "Sen araÅŸtÄ±rma analistisin. Web kaynaklarÄ±ndaki bilgileri Ã¶zetlersin.",
                    max_tokens=500
                )
                
                if analysis and "hatasÄ±" not in analysis.lower():
                    research_data.append({
                        'source': result['title'],
                        'url': result['url'],
                        'analysis': analysis
                    })
        
        # 4. Final rapor oluÅŸtur
        await self.websocket.send_json({
            "type": "progress", 
            "step": 0.9, 
            "message": "ğŸ“ Comprehensive research report yazÄ±lÄ±yor..."
        })
        
        # TÃ¼m analiz sonuÃ§larÄ±nÄ± birleÅŸtir
        combined_research = "\n\n".join([
            f"**Kaynak: {item['source']}**\nURL: {item['url']}\n{item['analysis']}"
            for item in research_data
        ])
        
        # EÄŸer hiÃ§ araÅŸtÄ±rma verisi yoksa, model bilgisiyle fallback yap
        if not research_data:
            fallback_prompt = f"""
'{topic}' konusu hakkÄ±nda mevcut bilgilerini kullanarak kapsamlÄ± bir analiz yap:

- Temel tanÄ±m ve kavramlar
- Ã–nemli Ã¶zellikler ve karakteristikler
- GÃ¼ncel durum ve geliÅŸmeler
- Gelecek beklentileri
- Pratik uygulamalar

DetaylÄ± ve bilgilendirici bir rapor hazÄ±rla.
"""
            fallback_research = await self.call_local_model(
                fallback_prompt,
                "Sen uzman araÅŸtÄ±rmacÄ±sÄ±sÄ±n. Konular hakkÄ±nda kapsamlÄ± analizler yaparsÄ±n.",
                max_tokens=2000
            )
            
            research_data.append({
                'source': 'AI Model Bilgi TabanÄ±',
                'url': 'N/A',
                'analysis': fallback_research
            })
            
            combined_research = f"**Kaynak: AI Model Bilgi TabanÄ±**\nURL: N/A\n{fallback_research}"
        
        final_prompt = f"""
AÅŸaÄŸÄ±daki araÅŸtÄ±rma sonuÃ§larÄ±nÄ± kullanarak '{topic}' konusu hakkÄ±nda TÃ¼rkÃ§e kapsamlÄ± bir rapor hazÄ±rla:

ARAÅTIRMA VERÄ°LERÄ°:
{combined_research}

RAPOR Ä°HTÄ°YAÃ‡LARI:
- Tamamen TÃ¼rkÃ§e yazÄ±lmÄ±ÅŸ olmasÄ± (hiÃ§ Ä°ngilizce kelime kullanma)
- Net giriÅŸ bÃ¶lÃ¼mÃ¼
- Ana bulgular ve Ã¶nemli geliÅŸmeler
- DetaylÄ± analiz ve deÄŸerlendirmeler
- SonuÃ§ ve gelecek Ã¶ngÃ¶rÃ¼leri  
- Markdown formatÄ±nda dÃ¼zenli yapÄ±

Ã–zellikle 2025 yÄ±lÄ±ndaki geliÅŸmelere odaklanarak gÃ¼ncel ve bilimsel bir rapor oluÅŸtur. TÃ¼m metni TÃ¼rkÃ§e yaz.
"""
        
        final_report = await self.call_local_model(
            final_prompt,
            "Sen uzman araÅŸtÄ±rmacÄ±sÄ±sÄ±n. Web araÅŸtÄ±rmasÄ± sonuÃ§larÄ±ndan kapsamlÄ±, profesyonel raporlar yazarsÄ±n.",
            max_tokens=4000
        )
        
        # 5. Dosya kaydet
        await self.websocket.send_json({
            "type": "progress", 
            "step": 0.95, 
            "message": "ğŸ’¾ Rapor masaÃ¼stÃ¼ne kaydediliyor..."
        })
        
        try:
            # MasaÃ¼stÃ¼ yolu - host masaÃ¼stÃ¼ne kaydet
            import os
            desktop_path = "/app/desktop"  # Docker'da host masaÃ¼stÃ¼ mount edildi
            research_path = "/app/research_results"  # Container iÃ§i results
            
            os.makedirs(desktop_path, exist_ok=True)
            os.makedirs(research_path, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()[:50]
            filename = f"{timestamp}_{safe_topic.replace(' ', '_')}_deep_research.md"
            
            # Hem masaÃ¼stÃ¼ne hem research_results'a kaydet
            desktop_filepath = os.path.join(desktop_path, filename)
            research_filepath = os.path.join(research_path, filename)
            
            report_header = f"""# Derin AraÅŸtÄ±rma Raporu: {topic}

**AraÅŸtÄ±rma Tarihi:** {datetime.now().strftime('%d %B %Y, %H:%M')}
**AraÅŸtÄ±rma TÃ¼rÃ¼:** Web TabanlÄ± Derin AraÅŸtÄ±rma
**KullanÄ±lan Model:** {self.model_name}
**Toplam Kaynak:** {len(research_data)} web kaynaÄŸÄ±
**Arama SorgularÄ±:** {len(search_queries)} farklÄ± sorgu

---

"""
            
            source_list = "\n".join([
                f"- [{item['source']}]({item['url']})"
                for item in research_data
            ])
            
            full_report = report_header + final_report + f"\n\n## Kaynaklar\n\n{source_list}"
            
            # MasaÃ¼stÃ¼ne kaydet
            try:
                with open(desktop_filepath, 'w', encoding='utf-8') as f:
                    f.write(full_report)
                await self.websocket.send_json({
                    "type": "progress", 
                    "step": 0.97, 
                    "message": f"âœ… Rapor masaÃ¼stÃ¼ne kaydedildi: {filename}"
                })
            except Exception as e:
                logger.error(f"Desktop save error: {e}")
            
            # Research results'a da kaydet
            try:
                with open(research_filepath, 'w', encoding='utf-8') as f:
                    f.write(full_report)
                await self.websocket.send_json({
                    "type": "progress", 
                    "step": 0.98, 
                    "message": f"âœ… Yedek kopya da kaydedildi"
                })
            except Exception as e:
                logger.error(f"Research results save error: {e}")
            
        except Exception as e:
            final_report += f"\n\n**Not:** Dosya kaydetme hatasÄ±: {str(e)}"
        
        await self.websocket.send_json({
            "type": "progress", 
            "step": 1.0, 
            "message": "ğŸ‰ Deep research tamamlandÄ±!"
        })
        
        return final_report